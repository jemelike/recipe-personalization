{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jemelike/recipe-personalization/blob/setup_improvements/recipe_gen/notebooks/example_train.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKBtTol4AQVK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "from itertools import chain\n",
        "from datetime import datetime\n",
        "import torch\n",
        "import torch.nn.init as init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def run_epoch(device, model, sampler, loss_compute, print_every, max_len,\n",
        "              clip=None, teacher_forcing=False, max_name_len=15, **tensor_kwargs):\n",
        "    \"\"\"\n",
        "    Run a single epoch\n",
        "\n",
        "    Arguments:\n",
        "        device {torch.device} -- Torch device on which to store/process data\n",
        "        model {nn.Module} -- Model to be trained/run\n",
        "        sampler {BatchSampler} -- Data sampler\n",
        "        loss_compute {funct} -- Function to compute loss for each batch\n",
        "        print_every {int} -- Log loss every k iterations\n",
        "        max_len {int} -- Maximum length / number of steps to unroll and predict\n",
        "\n",
        "    Keyword Arguments:\n",
        "        clip {float} -- Clip gradients to a maximum (default: {None})\n",
        "        teacher_forcing {bool} -- Whether to do teacher-forcing in training (default: {False})\n",
        "        max_name_len {int} -- Maximum # timesteps to unroll to predict name (default: {15})\n",
        "        **tensor_kwargs {torch.Tensor} -- Assorted tensors for fun and profit\n",
        "\n",
        "    Returns:\n",
        "        float -- Average loss across the epoch\n",
        "    \"\"\"\n",
        "    start = datetime.now()\n",
        "    total_tokens = 0\n",
        "    total_name_tokens = 0\n",
        "    total_loss = 0.0\n",
        "    total_name_loss = 0.0\n",
        "    print_tokens = 0\n",
        "\n",
        "    # Extract into tuples and list\n",
        "    tensor_names, base_tensors = zip(*tensor_kwargs.items())\n",
        "\n",
        "    # Iterate through batches in the epoch\n",
        "    for i, batch in enumerate(tqdm(sampler.epoch_batches(), total=sampler.n_batches), 1):\n",
        "        batch_users, items = [t.to(device) for t in batch]\n",
        "\n",
        "        # Fill out batch information\n",
        "        batch_map = dict(zip(\n",
        "            tensor_names,\n",
        "            get_batch_information_general(items, *base_tensors)\n",
        "        ))\n",
        "\n",
        "        # Logistics\n",
        "        this_batch_size = batch_map['steps_tensor'].size(0)\n",
        "        this_batch_num_tokens = (batch_map['steps_tensor'] != PAD_INDEX).data.sum().item()\n",
        "        this_batch_num_name_tokens = 0\n",
        "        this_batch_num_name_tokens = (batch_map['name_tensor'] != PAD_INDEX).data.sum().item()\n",
        "        name_targets = batch_map['name_tensor']\n",
        "\n",
        "        # Batch first\n",
        "        # Comparing out(token[t-1]) to token[t]\n",
        "        (log_probs, _), (name_log_probs, _) = model.forward(\n",
        "            device=device, inputs=(\n",
        "                batch_map['calorie_level_tensor'],\n",
        "                batch_map['name_tensor'],\n",
        "                batch_map['ingr_tensor']\n",
        "            ),\n",
        "            ingr_masks=batch_map['ingr_mask_tensor'],\n",
        "            targets=batch_map['steps_tensor'][:, :-1],\n",
        "            max_len=max_len-1,\n",
        "            start_token=START_INDEX,\n",
        "            teacher_forcing=teacher_forcing,\n",
        "            name_targets=name_targets[:, :-1],\n",
        "            max_name_len=max_name_len-1,\n",
        "            visualize=False\n",
        "        )\n",
        "        loss, name_loss = loss_compute(\n",
        "            log_probs, batch_map['steps_tensor'][:, 1:],\n",
        "            name_outputs=name_log_probs,\n",
        "            name_targets=name_targets[:, 1:],\n",
        "            norm=this_batch_size,\n",
        "            model=model,\n",
        "            clip=clip\n",
        "        )\n",
        "\n",
        "        total_loss += loss\n",
        "        total_name_loss += name_loss\n",
        "\n",
        "        # Logging\n",
        "        total_tokens += this_batch_num_tokens\n",
        "        total_name_tokens += this_batch_num_name_tokens\n",
        "        print_tokens += this_batch_num_tokens\n",
        "\n",
        "        if model.training and i % print_every == 0:\n",
        "            elapsed = datetime.now() - start\n",
        "            print(\"Epoch Step: {} LM Loss: {:.5f}; {}; Tokens/s: {:.3f}\".format(\n",
        "                i,\n",
        "                loss / this_batch_size,\n",
        "                'Name Loss: {:.5f}'.format(name_loss / this_batch_size) if name_loss else '',\n",
        "                print_tokens / elapsed.seconds\n",
        "            ))\n",
        "            start = datetime.now()\n",
        "            print_tokens = 0\n",
        "\n",
        "        del log_probs, name_log_probs\n",
        "\n",
        "    # Reshuffle the sampler\n",
        "    sampler.renew_indices()\n",
        "\n",
        "    if total_name_tokens > 0:\n",
        "        print('\\nName Perplexity: {}'.format(np.exp(total_name_loss / float(total_name_tokens))))\n",
        "\n",
        "    return np.exp(total_loss / float(total_tokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "start = datetime.now()\n",
        "USE_CUDA, DEVICE = get_device()\n",
        "\n",
        "# Filters\n",
        "MAX_NAME = 15\n",
        "MAX_INGR = 5\n",
        "MAX_INGR_TOK = 20\n",
        "MAX_STEP_TOK = 256\n",
        "\n",
        "# Reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Args\n",
        "data_dir = \"\"\n",
        "batch_size = 25\n",
        "vocab_emb_dim = 300\n",
        "calorie_emb_dim = 5\n",
        "ingr_emb_dim = args.ingr_emb_size\n",
        "hidden_size = 256\n",
        "n_layers = 2\n",
        "dropout = .2\n",
        "num_epochs = 50\n",
        "lr = \"1e-3\"\n",
        "print_every = 500\n",
        "exp_name = \"baseline\"\n",
        "save_folder = \"modle/baseline\"\n",
        "lr_annealing_rate = 0.9\n",
        "clip = None\n",
        "ingr_gru = False\n",
        "ingr_emb = True\n",
        "decode_name = False\n",
        "shared_proj = False\n",
        "n_teacher_forcing = None\n",
        "checkpoint_loc = None"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP91HxzZZO4QtqXsUCRkCHP",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 ('.env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "85cb9a3b3ea9d4403463f65d1f09163baef3f28ebdb64d5d95b32d0985fcfc43"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
